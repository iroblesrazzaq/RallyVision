import cv2
import torch
import sys
import time
import os
import numpy as np
from ultralytics import YOLO


class PoseExtractor:
    """
    A class that uses YOLOv8-pose model to extract raw numerical pose data
    from video segments and saves it to compressed .npz files.
    """
    
    def __init__(self, model_path='yolov8n-pose.pt'):
        """
        Initialize the PoseExtractor with a YOLOv8-pose model.
        
        Args:
            model_path (str): Path to the YOLOv8-pose model file.
                             Defaults to 'yolov8s-pose.pt'
        """
        # Try to use MPS if available, fallback to CPU
        if torch.backends.mps.is_available():
            self.device = "mps"
            print(f"Using device: {self.device.upper()} (MPS available)")
        else:
            self.device = "cpu"
            print(f"Using device: {self.device.upper()} (MPS not available, using CPU)")
        
        self.model_path = model_path
        self.model = YOLO(model_path)
        print(f"YOLOv8-pose model loaded successfully from: {model_path}")
    
    def extract_pose_data(self, video_path, confidence_threshold, start_time_seconds=0, duration_seconds=60, target_fps=15, annotations_csv=None):
        """
        Extract raw pose data from a video segment and save to .npz file.
        
        Args:
            video_path (str): Path to the input video file
            confidence_threshold (float): Confidence threshold for the model - no default
            start_time_seconds (int): Start time in seconds (default: 0)
            duration_seconds (int): Duration to process in seconds (default: 60)
            target_fps (int): Target frame rate for consistent temporal sampling (default: 15)
            annotations_csv (str): Path to CSV file with point annotations (optional)
            
        Returns:
            str: Path to the created .npz file
        """
        # Open the video file
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            print(f"Error: Could not open video file: {video_path}")
            return None
        
        # Get video properties
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # Calculate frame range
        start_frame = int(start_time_seconds * fps)
        end_frame = min(int(start_frame + (duration_seconds * fps)), total_frames)
        
        print(f"Processing frames {start_frame} to {end_frame} (Source FPS: {fps})")
        print(f"Target FPS: {target_fps}")
        
        # Calculate frame selection for target FPS
        frame_interval = fps / target_fps
        print(f"Frame interval: {frame_interval:.2f} frames")
        
        # Set video position to start frame
        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
        
        # Initialize list to store all frame data
        all_frames_data = []
        total_frames_to_process = end_frame - start_frame
        
        # Load annotations if provided
        annotations = None
        if annotations_csv and os.path.exists(annotations_csv):
            try:
                import pandas as pd
                annotations = pd.read_csv(annotations_csv)
                print(f"Loaded annotations from {annotations_csv}")
            except Exception as e:
                print(f"Warning: Could not load annotations from {annotations_csv}: {e}")
        
        print("Extracting pose data...")
        processed_frames = 0
        
        # Calculate which frames to process for target FPS
        target_frames = []
        frame_interval = fps / target_fps
        for i in range(total_frames_to_process):
            current_time = start_time_seconds + (i / fps)
            target_frame_index = int(current_time * target_fps)
            target_time = target_frame_index / target_fps
            
            # Check if this frame should be processed (closest to target time)
            if abs(current_time - target_time) <= (1 / target_fps) / 2:
                target_frames.append(i)
        
        print(f"Will process {len(target_frames)} frames out of {total_frames_to_process} total frames")
        
        for i in range(total_frames_to_process):
            ret, frame = cap.read()
            
            if not ret:
                print(f"Error reading frame {i + start_frame}")
                break
            
            # Check if we should process this frame
            if i in target_frames:
                # Run YOLOv8-pose model on the frame (no plotting)
                results = self.model(frame, verbose=False, device=self.device, conf=confidence_threshold, imgsz=1920)
                
                # Extract raw numerical data
                frame_data = {}
                
                if len(results) > 0 and results[0].boxes is not None:
                    # Extract bounding boxes
                    boxes = results[0].boxes.xyxy.cpu().numpy()  # x1, y1, x2, y2 format
                    frame_data['boxes'] = boxes
                    
                    # Extract keypoints
                    keypoints = results[0].keypoints.xy.cpu().numpy()  # [num_persons, num_keypoints, 2]
                    frame_data['keypoints'] = keypoints
                    
                    # Extract keypoint confidences
                    keypoint_conf = results[0].keypoints.conf.cpu().numpy()  # [num_persons, num_keypoints]
                    frame_data['conf'] = keypoint_conf
                else:
                    # No detections in this frame
                    frame_data['boxes'] = np.array([])
                    frame_data['keypoints'] = np.array([])
                    frame_data['conf'] = np.array([])
                
                # Add annotation status (-100 for skipped frames, 0/1 for annotated frames)
                frame_data['annotation_status'] = 0  # Default to not in play
                
                # Check if this frame is in any annotated point interval
                if annotations is not None:
                    frame_time = start_time_seconds + (i / fps)
                    for _, row in annotations.iterrows():
                        start_time = row['start_frame'] / target_fps  # Assuming start_frame is in frame numbers
                        end_time = row['end_frame'] / target_fps      # Assuming end_frame is in frame numbers
                        if start_time <= frame_time <= end_time:
                            frame_data['annotation_status'] = 1  # In play
                            break
                
                all_frames_data.append(frame_data)
                processed_frames += 1
            else:
                # Skip this frame - add empty data with annotation status -100
                frame_data = {
                    'boxes': np.array([]),
                    'keypoints': np.array([]),
                    'conf': np.array([]),
                    'annotation_status': -100  # Skipped frame
                }
                all_frames_data.append(frame_data)
            
            # Update progress bar
            progress = (i + 1) / total_frames_to_process * 100
            print(f"\rProgress: [{('=' * int(progress/2)).ljust(50)}] {progress:.1f}% (processed {processed_frames} frames)", end='', flush=True)
        
        print()  # New line after progress bar
        
        # Cleanup
        cap.release()
        
        print(f"âœ“ Successfully extracted pose data from {len(all_frames_data)} frames")
        
        # Save data to .npz file
        # Extract model size from model path (e.g., "yolov8s-pose.pt" -> "s")
        if 'yolov8' in self.model_path:
            model_size = self.model_path.split('yolov8')[1].split('-')[0]
        else:
            model_size = 's'  # Default fallback
        
        # Create subdirectory with model size, confidence threshold, fps, and time range
        subdir_name = f"yolo{model_size}_{confidence_threshold}conf_{target_fps}fps_{start_time_seconds}s_to_{start_time_seconds + duration_seconds}s"
        output_dir = os.path.join("pose_data", "unfiltered", subdir_name)
        os.makedirs(output_dir, exist_ok=True)
        
        # Construct descriptive filename
        base_name = os.path.splitext(os.path.basename(video_path))[0]
        output_filename = f"{base_name}_posedata_{start_time_seconds}s_to_{start_time_seconds + duration_seconds}s_yolo{model_size}.npz"
        output_path = os.path.join(output_dir, output_filename)
        
        # Save compressed data
        np.savez_compressed(output_path, frames=all_frames_data)
        print(f"âœ“ Pose data saved to: {output_path}")
        
        return output_path


if __name__ == "__main__":
    # Parse command line arguments
    if len(sys.argv) >= 3:
        start_time = int(sys.argv[1])
        duration = int(sys.argv[2])
        target_fps = int(sys.argv[3]) if len(sys.argv) > 3 else 15
        confidence_threshold = float(sys.argv[4]) if len(sys.argv) > 4 else 0.05
        video_path = sys.argv[5] if len(sys.argv) > 5 else "raw_videos/Monica Greene unedited tennis match play.mp4"
        model_size = sys.argv[6] if len(sys.argv) > 6 else "s"
        annotations_csv = sys.argv[7] if len(sys.argv) > 7 else None
    else:
        start_time = 0
        duration = 10  # Default to 10 seconds for testing
        target_fps = 15
        confidence_threshold = 0.05
        video_path = "raw_videos/Monica Greene unedited tennis match play.mp4"
        model_size = "s"
        annotations_csv = None
    
    # Start timing
    script_start_time = time.time()
    
    # Construct model path based on model size
    model_path = f"yolov8{model_size}-pose.pt"
    
    print("Initializing PoseExtractor...")
    pose_extractor = PoseExtractor(model_path=model_path)
    
    print(f"Processing video: {video_path}")
    print(f"Start time: {start_time}s, Duration: {duration}s, Target FPS: {target_fps}, Model: {model_path}")
    if annotations_csv:
        print(f"Annotations CSV: {annotations_csv}")
    
    # Extract pose data
    output_path = pose_extractor.extract_pose_data(
        video_path=video_path,
        start_time_seconds=start_time,
        duration_seconds=duration,
        target_fps=target_fps,
        confidence_threshold=confidence_threshold,
        annotations_csv=annotations_csv

    )
    
    if output_path is None:
        print("Failed to extract pose data. Exiting.")
        exit()
    
    # Calculate and print total runtime
    script_end_time = time.time()
    total_runtime = script_end_time - script_start_time
    
    print(f"\nðŸŽ¯ Total runtime: {total_runtime:.2f} seconds ({total_runtime/60:.2f} minutes)")
