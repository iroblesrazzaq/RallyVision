{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c7c368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Training script for Tennis Point Detection LSTM Model\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tennis_dataset import TennisDataset\n",
    "from lstm_model_arch import TennisPointLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f796e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "bidirectional = True\n",
    "pos_weight = 4.0\n",
    "early_stopping_patience = 10\n",
    "early_stopping_threshold = None\n",
    "checkpoint_dir = 'checkpoints'\n",
    "log_dir = 'logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d444e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TennisDataset('data/train.h5')\n",
    "val_dataset = TennisDataset('data/val.h5')\n",
    "test_dataset = TennisDataset('data/test.h5')\n",
    "\n",
    "print(f\"  Training samples: {len(train_dataset)}\")\n",
    "print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "print(f\"  Test samples: {len(test_dataset)}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d251e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, test_loader, \n",
    "                num_epochs=50, learning_rate=0.001, batch_size=32,\n",
    "                pos_weight=None, early_stopping_patience=10, early_stopping_threshold=None, weighted_accuracy=True  ,\n",
    "                checkpoint_dir='checkpoints', log_dir='logs'):\n",
    "    \"\"\"\n",
    "    Train the tennis point detection model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to train\n",
    "        train_loader (DataLoader): Training data loader\n",
    "        val_loader (DataLoader): Validation data loader\n",
    "        test_loader (DataLoader): Test data loader\n",
    "        num_epochs (int): Number of epochs to train\n",
    "        learning_rate (float): Learning rate for optimizer\n",
    "        batch_size (int): Batch size\n",
    "        pos_weight (float): Positive class weight for weighted loss\n",
    "        early_stopping_patience (int): Number of epochs to wait before early stopping\n",
    "        early_stopping_threshold (float): Validation loss threshold for early stopping (optional)\n",
    "        checkpoint_dir (str): Directory to save model checkpoints\n",
    "        log_dir (str): Directory to save logs\n",
    "    \"\"\"\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    if pos_weight is not None and pos_weight > 0:\n",
    "        # Use weighted BCEWithLogitsLoss\n",
    "        pos_weight_tensor = torch.tensor([pos_weight]).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "        print(f\"Using weighted loss with positive weight: {pos_weight}\")\n",
    "    else:\n",
    "        # Standard BCE loss\n",
    "        criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "        print(\"Using standard BCE loss\")\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-3)\n",
    "    \n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_accuracy = 0.0\n",
    "    epochs_since_improvement = 0\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (sequences, labels) in enumerate(train_loader):\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(sequences)\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            batch_size, seq_length, _ = outputs.shape\n",
    "            outputs_reshaped = outputs.view(batch_size * seq_length, 1)\n",
    "            labels_reshaped = labels.view(batch_size * seq_length, 1).float()\n",
    "            \n",
    "            # Calculate loss\n",
    "            if pos_weight is not None and pos_weight > 0:\n",
    "                # For weighted loss, we use raw logits\n",
    "                loss = criterion(outputs_reshaped, labels_reshaped)\n",
    "            else:\n",
    "                # Standard BCELoss\n",
    "                loss = criterion(outputs_reshaped, labels_reshaped)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            train_loss += loss.item()\n",
    "            # Calculate accuracy using probabilities\n",
    "            predictions = (torch.sigmoid(outputs_reshaped) > 0.5).float()\n",
    "            train_correct += (predictions == labels_reshaped).sum().item()\n",
    "            train_total += labels_reshaped.numel()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'  Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Calculate average training loss and accuracy\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = train_correct / train_total\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_accuracy = evaluate_model(model, val_loader, criterion, device, \n",
    "                                              pos_weight if pos_weight is not None and pos_weight > 0 else None,\n",
    "                                              weighted_accuracy=True)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        # Check if this is the best model\n",
    "        is_best = val_accuracy > best_val_accuracy\n",
    "        if is_best:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            \n",
    "        # Early stopping check\n",
    "        # Check if validation loss has improved\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_since_improvement = 0\n",
    "        else:\n",
    "            epochs_since_improvement += 1\n",
    "            \n",
    "        # Check early stopping conditions\n",
    "        should_stop = False\n",
    "        \n",
    "        # Patience-based early stopping\n",
    "        print(f\"Early stopping patience: {early_stopping_patience}\")\n",
    "        if early_stopping_patience > 0 and epochs_since_improvement >= early_stopping_patience:\n",
    "            print(f\"Early stopping: No improvement in validation loss for {early_stopping_patience} epochs\")\n",
    "            should_stop = True\n",
    "            \n",
    "        # Threshold-based early stopping\n",
    "        if early_stopping_threshold is not None and val_loss <= early_stopping_threshold:\n",
    "            print(f\"Early stopping: Validation loss {val_loss:.4f} reached threshold {early_stopping_threshold:.4f}\")\n",
    "            should_stop = True\n",
    "            \n",
    "        # Save checkpoint\n",
    "        save_checkpoint(model, optimizer, epoch+1, avg_train_loss, val_loss, val_accuracy, \n",
    "                       checkpoint_dir, is_best)\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] - '\n",
    "              f'Train Loss: {avg_train_loss:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, '\n",
    "              f'Train Acc: {train_accuracy:.4f}, '\n",
    "              f'Val Acc: {val_accuracy:.4f}, '\n",
    "              f'Time: {epoch_time:.2f}s')\n",
    "        \n",
    "        # Save training history\n",
    "        history = {\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'train_accuracies': train_accuracies,\n",
    "            'val_accuracies': val_accuracies,\n",
    "            'best_val_accuracy': best_val_accuracy,\n",
    "            'best_val_loss': best_val_loss\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(log_dir, 'training_history.json'), 'w') as f:\n",
    "            json.dump(history, f)\n",
    "            \n",
    "        # Early stopping\n",
    "        if should_stop:\n",
    "            print(f\"Stopping training at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Training completed\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTraining completed in {total_time:.2f} seconds\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device,\n",
    "                                            pos_weight if pos_weight is not None and pos_weight > 0 else None,\n",
    "                                            weighted_accuracy=True)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Save final test results\n",
    "    test_results = {\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': test_accuracy\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(log_dir, 'test_results.json'), 'w') as f:\n",
    "        json.dump(test_results, f)\n",
    "    \n",
    "    # Plot training curves\n",
    "    plot_path = os.path.join(log_dir, 'training_curves.png')\n",
    "    plot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies, plot_path)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3812bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, data_loader, criterion, device, pos_weight=None, weighted_accuracy=True):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a given dataset.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to evaluate\n",
    "        data_loader (DataLoader): DataLoader for the dataset\n",
    "        criterion (nn.Module): Loss function\n",
    "        device (torch.device): Device to use for computation\n",
    "        pos_weight (float): Positive class weight for weighted loss (optional)\n",
    "        weighted_accuracy (bool): Whether to weight in-point predictions 4x more than not-in-point\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (average_loss, accuracy)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    total_pos_correct = 0  # Track positive class correct predictions\n",
    "    total_pos_samples = 0  # Track positive class samples\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in data_loader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(sequences)\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            batch_size, seq_length, _ = outputs.shape\n",
    "            outputs_reshaped = outputs.view(batch_size * seq_length, 1)\n",
    "            labels_reshaped = labels.view(batch_size * seq_length, 1).float()\n",
    "            \n",
    "            # Calculate loss\n",
    "            if pos_weight is not None and pos_weight > 0:\n",
    "                # For weighted evaluation, we need to use BCEWithLogitsLoss\n",
    "                pos_weight_tensor = torch.tensor([pos_weight]).to(device)\n",
    "                loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "                # We need raw logits for BCEWithLogitsLoss\n",
    "                loss = loss_fn(outputs_reshaped, labels_reshaped)\n",
    "            else:\n",
    "                loss = criterion(outputs_reshaped, labels_reshaped)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy using probabilities\n",
    "            predictions = (torch.sigmoid(outputs_reshaped) > 0.5).float()\n",
    "            correct_predictions = (predictions == labels_reshaped)\n",
    "            \n",
    "            # Track overall accuracy\n",
    "            total_correct += correct_predictions.sum().item()\n",
    "            total_samples += labels_reshaped.numel()\n",
    "            \n",
    "            # Track positive class accuracy (weighted 4x)\n",
    "            pos_mask = (labels_reshaped == 1)\n",
    "            if pos_mask.sum() > 0:\n",
    "                total_pos_correct += correct_predictions[pos_mask].sum().item()\n",
    "                total_pos_samples += pos_mask.sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    \n",
    "    # Calculate weighted accuracy: 4x weight for positive class\n",
    "    if weighted_accuracy and total_pos_samples > 0:\n",
    "        pos_accuracy = total_pos_correct / total_pos_samples\n",
    "        overall_accuracy = total_correct / total_samples\n",
    "        # Weighted accuracy: 4 parts positive accuracy, 1 part negative accuracy\n",
    "        weighted_accuracy = (4 * pos_accuracy + overall_accuracy) / 5\n",
    "        accuracy = weighted_accuracy\n",
    "    else:\n",
    "        accuracy = total_correct / total_samples\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932675fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(train_losses, val_losses, train_accuracies, val_accuracies, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot training and validation curves.\n",
    "    \n",
    "    Args:\n",
    "        train_losses (list): Training losses over epochs\n",
    "        val_losses (list): Validation losses over epochs\n",
    "        train_accuracies (list): Training accuracies over epochs\n",
    "        val_accuracies (list): Validation accuracies over epochs\n",
    "        save_path (str): Path to save the plot (optional)\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Plot loss curves\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    ax1.plot(epochs, train_losses, label='Train Loss', marker='o')\n",
    "    ax1.plot(epochs, val_losses, label='Validation Loss', marker='s')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot accuracy curves\n",
    "    ax2.plot(epochs, train_accuracies, label='\n",
    "uracy', marker='o')\n",
    "    ax2.plot(epochs, val_accuracies, label='Validation Accuracy', marker='s')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Training curves saved to {save_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6725d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, train_loss, val_loss, val_accuracy, checkpoint_dir, is_best=False):\n",
    "    \"\"\"\n",
    "    Save model checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Model to save\n",
    "        optimizer (optim.Optimizer): Optimizer to save\n",
    "        epoch (int): Current epoch\n",
    "        train_loss (float): Current training loss\n",
    "        val_loss (float): Current validation loss\n",
    "        val_accuracy (float): Current validation accuracy\n",
    "        checkpoint_dir (str): Directory to save checkpoints\n",
    "        is_best (bool): Whether this is the best model so far\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'val_accuracy': val_accuracy\n",
    "    }\n",
    "    \n",
    "    # Save regular checkpoint\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pth')\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if is_best:\n",
    "        best_path = os.path.join(checkpoint_dir, 'best_model.pth')\n",
    "        torch.save(checkpoint, best_path)\n",
    "        print(f\"Best model saved to {best_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da9e907",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TennisPointLSTM(\n",
    "    input_size=360,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    "    bidirectional=bidirectional,\n",
    "    return_logits=(pos_weight is not None and pos_weight > 0)  #\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2db58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=epochs,\n",
    "    learning_rate=lr,\n",
    "    batch_size=batch_size,\n",
    "    pos_weight=pos_weight if pos_weight > 0 else None,\n",
    "    early_stopping_patience=early_stopping_patience,\n",
    "    early_stopping_threshold=early_stopping_threshold,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    log_dir=log_dir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1a8c4f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da725a31",
   "metadata": {},
   "source": [
    "# test set eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7826244",
   "metadata": {},
   "source": [
    "weighted accuracy + loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82076305",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "pos_weight_tensor = torch.tensor([pos_weight]).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "\n",
    "loaded_model = TennisPointLSTM(\n",
    "    input_size=360,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    "    bidirectional=bidirectional,\n",
    "    return_logits=(pos_weight is not None and pos_weight > 0)  #\n",
    ")\n",
    "# Load checkpoint and extract model state dict\n",
    "checkpoint = torch.load('checkpoints/best_model.pth')\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "loaded_model.eval()\n",
    "\n",
    "\n",
    "\n",
    "test_loss, test_accuracy = evaluate_model(loaded_model, test_loader, criterion, device,\n",
    "                                        pos_weight if pos_weight is not None and pos_weight > 0 else None,\n",
    "                                        weighted_accuracy=True)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e99c987",
   "metadata": {},
   "source": [
    "unweighted acc + loss - debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7f36f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device,\n",
    "                                        None,\n",
    "                                        weighted_accuracy=True)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3941600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tennis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
