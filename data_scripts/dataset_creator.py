"""
Dataset Creator for Tennis Point Detection LSTM Model

This module creates train/validation/test splits from feature-engineered tennis data,
generates overlapping sequences, and saves them in an efficient format for training.
"""

import numpy as np
import h5py
import os
import glob
import json
from pathlib import Path

class TennisDatasetCreator:
    """
    Creates train/validation/test datasets from feature-engineered tennis data.
    
    Strategy:
    1. Temporal splitting per video to prevent data leakage
    2. Sequence generation with 50% overlap
    3. Efficient HDF5 storage for large datasets
    """
    
    def __init__(self, feature_dir, output_dir, splits=(0.765, 0.085, 0.15)):
        """
        Initialize dataset creator.
        
        Args:
            feature_dir (str): Directory containing feature NPZ files
            output_dir (str): Directory to save processed datasets
            splits (tuple): (train_ratio, val_ratio, test_ratio)
        """
        self.feature_dir = Path(feature_dir)
        self.output_dir = Path(output_dir)
        self.train_ratio, self.val_ratio, self.test_ratio = splits
        
        # Create output directory
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Sequence parameters
        self.sequence_length = 150  # 10 seconds at 15 FPS
        self.overlap = 75  # 50% overlap
        self.num_features = 360  # Updated from 288 to match actual feature vectors
        
    def _create_temporal_splits(self, features, targets):
        """
        Split video data temporally to prevent data leakage.
        
        Args:
            features (np.array): Shape (N, num_features)
            targets (np.array): Shape (N,)
            
        Returns:
            dict: Dictionary with 'train', 'val', 'test' splits
        """
        total_frames = len(features)
        
        # Calculate split indices (temporally ordered)
        test_start = int(total_frames * (1 - self.test_ratio))
        val_start = int(total_frames * (1 - self.test_ratio - self.val_ratio))
        
        splits = {
            'train': {
                'features': features[:val_start],
                'targets': targets[:val_start],
                'frame_range': (0, val_start)
            },
            'val': {
                'features': features[val_start:test_start],
                'targets': targets[val_start:test_start],
                'frame_range': (val_start, test_start)
            },
            'test': {
                'features': features[test_start:],
                'targets': targets[test_start:],
                'frame_range': (test_start, total_frames)
            }
        }
        
        return splits
    
    def _create_sequences_with_overlap(self, features, targets):
        """
        Create overlapping sequences from temporal splits.
        
        Args:
            features (np.array): Shape (N, num_features)
            targets (np.array): Shape (N,)
            
        Returns:
            tuple: (sequences, sequence_targets) where each is a list of arrays
        """
        sequences = []
        sequence_targets = []
        
        start_idx = 0
        sequence_count = 0
        
        while start_idx + self.sequence_length <= len(features):
            # Extract sequence
            seq_features = features[start_idx:start_idx + self.sequence_length]
            seq_targets = targets[start_idx:start_idx + self.sequence_length]
            
            sequences.append(seq_features)
            sequence_targets.append(seq_targets)
            
            sequence_count += 1
            
            # Move to next sequence
            start_idx += self.overlap
            
            # Handle edge case: if next sequence would extend beyond data
            if start_idx + self.sequence_length > len(features):
                break
        
        print(f"  Generated {sequence_count} sequences from {len(features)} frames")
        
        return sequences, sequence_targets
    
    def _save_split_to_hdf5(self, sequences, targets, split_name):
        """
        Save sequences to HDF5 file.
        
        Args:
            sequences (list): List of sequence arrays
            targets (list): List of target arrays
            split_name (str): Name of split ('train', 'val', 'test')
        """
        if not sequences:
            print(f"  No sequences to save for {split_name}")
            return
            
        # Convert to numpy arrays
        sequences_array = np.array(sequences)
        targets_array = np.array(targets)
        
        # Save to HDF5
        output_file = self.output_dir / f"{split_name}.h5"
        
        with h5py.File(output_file, 'w') as f:
            f.create_dataset('features', data=sequences_array, compression='gzip')
            f.create_dataset('targets', data=targets_array, compression='gzip')
            
        print(f"  Saved {len(sequences)} sequences to {output_file}")
        print(f"    Features shape: {sequences_array.shape}")
        print(f"    Targets shape: {targets_array.shape}")
    
    def process_single_video(self, npz_file):
        """
        Process a single video's feature file.
        
        Args:
            npz_file (str): Path to feature NPZ file
            
        Returns:
            dict: Processing statistics
        """
        print(f"Processing {os.path.basename(npz_file)}")
        
        # Load feature data
        try:
            data = np.load(npz_file, allow_pickle=True)
            features = data['features']
            targets = data['targets']
        except Exception as e:
            print(f"  Error loading {npz_file}: {e}")
            return None
            
        print(f"  Loaded {len(features)} feature vectors")
        
        # Create temporal splits
        splits = self._create_temporal_splits(features, targets)
        
        # Process each split
        split_stats = {}
        for split_name, split_data in splits.items():
            split_features = split_data['features']
            split_targets = split_data['targets']
            
            if len(split_features) == 0:
                print(f"  Skipping {split_name} split (no data)")
                split_stats[split_name] = {'sequences': 0, 'frames': 0}
                continue
                
            print(f"  Creating sequences for {split_name} split "
                  f"({len(split_features)} frames)")
            
            # Create sequences
            sequences, sequence_targets = self._create_sequences_with_overlap(
                split_features, split_targets)
            
            split_stats[split_name] = {
                'sequences': len(sequences),
                'frames': len(split_features),
                'frame_range': split_data['frame_range']
            }
            
            # Save sequences (will be combined with other videos later)
            if sequences:
                self._save_individual_split(sequences, sequence_targets, 
                                          split_name, npz_file)
        
        return {
            'video': os.path.basename(npz_file),
            'total_frames': len(features),
            'splits': split_stats
        }
    
    def _save_individual_split(self, sequences, targets, split_name, npz_file):
        """
        Save sequences from a single video to split-specific file.
        
        Args:
            sequences (list): List of sequence arrays
            targets (list): List of target arrays
            split_name (str): Name of split
            npz_file (str): Source NPZ file path
        """
        if not sequences:
            return
            
        # Create split directory
        split_dir = self.output_dir / split_name
        split_dir.mkdir(exist_ok=True)
        
        # Get video name
        video_name = Path(npz_file).stem.replace('_features', '')
        
        # Save to individual file
        output_file = split_dir / f"{video_name}.h5"
        
        sequences_array = np.array(sequences)
        targets_array = np.array(targets)
        
        with h5py.File(output_file, 'w') as f:
            f.create_dataset('features', data=sequences_array, compression='gzip')
            f.create_dataset('targets', data=targets_array, compression='gzip')
    
    def combine_splits(self):
        """
        Combine individual video splits into final dataset files.
        """
        print("Combining individual splits into final datasets...")
        
        for split_name in ['train', 'val', 'test']:
            split_dir = self.output_dir / split_name
            
            if not split_dir.exists():
                print(f"  No {split_name} directory found")
                continue
                
            # Find all HDF5 files for this split
            h5_files = list(split_dir.glob("*.h5"))
            
            if not h5_files:
                print(f"  No files found for {split_name} split")
                continue
                
            print(f"  Combining {len(h5_files)} files for {split_name} split")
            
            # Load and combine all sequences
            all_features = []
            all_targets = []
            
            for h5_file in h5_files:
                try:
                    with h5py.File(h5_file, 'r') as f:
                        features = f['features'][:]
                        targets = f['targets'][:]
                        all_features.append(features)
                        all_targets.append(targets)
                except Exception as e:
                    print(f"    Error loading {h5_file}: {e}")
                    continue
            
            if not all_features:
                print(f"  No data to combine for {split_name} split")
                continue
                
            # Concatenate all sequences
            combined_features = np.concatenate(all_features, axis=0)
            combined_targets = np.concatenate(all_targets, axis=0)
            
            # Save combined dataset
            output_file = self.output_dir / f"{split_name}.h5"
            with h5py.File(output_file, 'w') as f:
                f.create_dataset('features', data=combined_features, compression='gzip')
                f.create_dataset('targets', data=combined_targets, compression='gzip')
            
            print(f"  Saved combined {split_name} dataset:")
            print(f"    Features shape: {combined_features.shape}")
            print(f"    Targets shape: {combined_targets.shape}")
            print(f"    Total sequences: {len(combined_features)}")
    
    def create_dataset(self):
        """
        Main method to create the complete dataset.
        
        Returns:
            dict: Dataset creation statistics
        """
        print("Creating tennis point detection dataset...")
        print(f"Feature directory: {self.feature_dir}")
        print(f"Output directory: {self.output_dir}")
        print(f"Splits: train={self.train_ratio}, val={self.val_ratio}, test={self.test_ratio}")
        print(f"Sequence length: {self.sequence_length}, overlap: {self.overlap}")
        
        # Find all feature NPZ files
        npz_files = list(self.feature_dir.glob("*_features.npz"))
        
        if not npz_files:
            print("No feature NPZ files found!")
            return None
            
        print(f"Found {len(npz_files)} feature files")
        
        # Process each video
        all_stats = []
        for npz_file in npz_files:
            stats = self.process_single_video(str(npz_file))
            if stats:
                all_stats.append(stats)
        
        # Combine splits
        self.combine_splits()
        
        # Save dataset info
        dataset_info = {
            'splits': {
                'train_ratio': self.train_ratio,
                'val_ratio': self.val_ratio,
                'test_ratio': self.test_ratio
            },
            'sequence_params': {
                'length': self.sequence_length,
                'overlap': self.overlap
            },
            'video_stats': all_stats,
            'total_videos': len(npz_files)
        }
        
        info_file = self.output_dir / "dataset_info.json"
        with open(info_file, 'w') as f:
            json.dump(dataset_info, f, indent=2)
        
        print(f"Dataset creation complete!")
        print(f"Dataset info saved to {info_file}")
        
        return dataset_info

def main():
    """Example usage of TennisDatasetCreator."""
    # Initialize creator
    creator = TennisDatasetCreator(
        feature_dir="pose_data/features/yolos_0.25conf_15fps_15s_to_99999s",
        output_dir="processed_data",
        splits=(0.765, 0.085, 0.15)
    )
    
    # Create dataset
    stats = creator.create_dataset()
    
    if stats:
        print("\nDataset Statistics:")
        print(json.dumps(stats, indent=2))

if __name__ == "__main__":
    main()