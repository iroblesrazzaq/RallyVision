<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RallyVision — Free Tennis Match Segmentation</title>
    <meta name="description" content="Open-source AI tool that extracts only the rallies from tennis match recordings.">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="page">
        <header class="header">
            <h1>RallyVision</h1>
            <p class="tagline">Free, open-source tennis match segmentation.<br>Runs entirely on your machine.</p>
            <a href="https://github.com/iroblesrazzaq/RallyVision" class="github-link" target="_blank">GitHub ↗</a>
        </header>

        <nav class="nav">
            <a href="#about">About</a>
            <a href="#motivation">Motivation</a>
            <a href="#how-it-works">How It Works</a>
            <a href="#challenges">Challenges</a>
            <a href="#future">Future</a>
            <a href="#install">Install</a>
        </nav>

        <main class="content">
            <!-- About -->
            <section id="about">
                <h2>What It Does</h2>
                <p>RallyVision takes a full tennis match recording and extracts only the points—removing the dead time between rallies. Drop in a 2-hour match video, get back a condensed video containing just the action, plus an optional CSV with timestamps for each point.</p>
                <p>It uses a pipeline of computer vision and deep learning: court detection, YOLOv8 pose estimation, feature engineering, and a bidirectional LSTM that predicts frame-by-frame whether play is happening.</p>
            </section>

            <!-- Motivation -->
            <section id="motivation">
                <h2>Motivation</h2>
                <p>Watching yourself play is one of the best ways to improve at tennis. But only about 25% of a recorded match is actually "in-point"—the rest is dead time. Scrubbing through footage to find the actual rallies is tedious.</p>
                <p>Software like SwingVision solves this, but limits you to 2 hours/month of free segmentation before pushing an expensive subscription. While SwingVision offers many analytics, match segmentation is one of their most popular features.</p>
                <p><strong>I believe match segmentation should be free for anyone.</strong> That's why RallyVision is open-source and runs locally—no cloud costs, no subscriptions.</p>
                <p>The trade-off: video inference is computationally expensive, so processing takes time on a laptop. Here are benchmarks from my MacBook Pro M2:</p>
                
                <table>
                    <thead>
                        <tr>
                            <th>Model</th>
                            <th>GPU (MPS)</th>
                            <th>CPU</th>
                            <th>Speedup</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>YOLO nano</td>
                            <td>~0.8× video length</td>
                            <td>~3× video length</td>
                            <td>3.75×</td>
                        </tr>
                        <tr>
                            <td>YOLO small</td>
                            <td>~1.5× video length</td>
                            <td>~7× video length</td>
                            <td>4.7×</td>
                        </tr>
                    </tbody>
                </table>
                <p class="caption">GPU acceleration provides ~4× speedup. Use CUDA or MPS if available.</p>
            </section>

            <!-- How It Works -->
            <section id="how-it-works">
                <h2>How It Works</h2>
                
                <h3>1. Court Detection</h3>
                <p>Classical CV methods detect court lines, then heuristics identify the baseline and sidelines based on line angles. This creates a mask of the playable area, ensuring we only track players on the relevant court—ignoring people on adjacent courts.</p>
                
                <h3>2. Pose Extraction</h3>
                <p>YOLOv8 pose models run on 15fps downsampled video, extracting bounding boxes and keypoint (joint) positions for each player.</p>
                
                <h3>3. Feature Engineering</h3>
                <p>Filter bounding boxes to those inside the court mask. Compute features: limb lengths, joint velocities, and accelerations.</p>
                
                <h3>4. LSTM Inference</h3>
                <p>Overlapping 20-second chunks feed into a bidirectional LSTM, which outputs per-frame probabilities of "in point" vs "out of point."</p>
                
                <h3>5. Output Generation</h3>
                <p>Average the signal, apply smoothing and hysteresis filtering to get discrete start/end times. Write the "in-point" segments to the output video.</p>
            </section>

            <!-- Challenges -->
            <section id="challenges">
                <h2>Challenges</h2>
                <p>The main challenges: finding a data modality with consistent signal, and building a model efficient enough to run locally.</p>
                
                <h3>Choosing the Data Modality</h3>
                <p><strong>Raw video:</strong> Infeasible. Training CNNs or video transformers would require far more data than I could access.</p>
                <p><strong>Audio:</strong> Unreliable. Multiple courts make it hard to isolate sounds; not all shots are audible; some recordings lack audio entirely.</p>
                <p><strong>Ball tracking:</strong> Possible, but fails with occlusion or when ball and court colors blend.</p>
                <p><strong>Player poses:</strong> This is what I chose. Points require distinct movement patterns. Players are consistently visible during rallies. And pose extraction is cheap—YOLOv8-pose models are only 3–12M parameters.</p>
                
                <h3>Training Data</h3>
                <p>No public datasets exist for this task. <strong>I manually annotated ~8 hours of match footage.</strong> Building a working model on so little data required careful feature engineering—proof that clever approaches can match brute-force big data methods.</p>
                
                <h3>Limitations</h3>
                <p>If court detection fails (low light, bad camera angle) and extra people appear on screen, the pose detector may track wrong individuals, producing unreliable output. The model may also struggle with videos that differ significantly from training data. More data collection will improve this.</p>
            </section>

            <!-- Future -->
            <section id="future">
                <h2>Future Improvements</h2>
                <ul>
                    <li><strong>Model distillation:</strong> Fine-tune YOLO nano on YOLO large outputs to close the accuracy gap while keeping fast inference.</li>
                    <li><strong>More data:</strong> Expand training data with additional hand-labeled examples and model-generated pseudo-labels.</li>
                    <li><strong>Attention mechanisms:</strong> Upgrade to LSTM with attention to learn which parts of each 20-second window matter most.</li>
                    <li><strong>Rally detection:</strong> Train a separate model for rally-only detection (current model has learned to associate serves/returns with point starts).</li>
                </ul>
                <p class="note">Have ideas? PRs welcome—I'm open to feedback and contributions.</p>
            </section>

            <!-- Install -->
            <section id="install">
                <h2>Installation</h2>
                <p class="requirements">Requires Python 3.10+. GPU optional (MPS/CUDA supported).</p>
<pre><code><span class="comment"># Clone and install</span>
git clone https://github.com/iroblesrazzaq/RallyVision.git
cd RallyVision
pip install .</code></pre>
                <p class="caption">Models included in repo. YOLO weights auto-download on first run.</p>
            </section>

            <!-- Usage -->
            <section id="usage">
                <h2>Usage</h2>
                
                <h3>GUI</h3>
                <p>Launch the browser interface:</p>
<pre><code>rallyvision gui</code></pre>
                <p>Drag and drop your video, adjust settings, download results.</p>
                
                <h3>CLI</h3>
<pre><code><span class="comment"># Basic</span>
rallyvision --video "match.mp4"

<span class="comment"># With CSV export</span>
rallyvision --video "match.mp4" --write-csv

<span class="comment"># Custom output</span>
rallyvision --video "match.mp4" --output-dir "./processed"</code></pre>
                <p>See <a href="https://github.com/iroblesrazzaq/RallyVision#readme" target="_blank">README</a> for full CLI options.</p>
            </section>
        </main>

        <footer class="footer">
            <a href="https://github.com/iroblesrazzaq/RallyVision" target="_blank">GitHub</a>
            <span>·</span>
            <a href="https://github.com/iroblesrazzaq/RallyVision/issues" target="_blank">Issues</a>
            <span>·</span>
            <span>MIT License</span>
        </footer>
    </div>
</body>
</html>
