{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daf8009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file to run inference on an entire feature npz file, then apply postprocessing steps\n",
    "# to output final start_time,end_time csv file\n",
    "# %%\n",
    "import numpy as np\n",
    "import torch\n",
    "from lstm_model_arch import TennisPointLSTM\n",
    "import scipy.ndimage\n",
    "import joblib\n",
    "from typing import Optional\n",
    "\n",
    "\"\"\"\n",
    "My test.py file is my current evaluation file. however, it just looks at sequences, not the whole video. \n",
    "I need to further establish my post processing pipeline. The final output of my pipeline should be \n",
    "a csv of start_time, end_times, which i can compare to the annotated targets. \n",
    "For now, we will use the same gaussian smoothing and hysteresis filtering that we're using in the test.py file. \n",
    "\n",
    "Your task is to write a new file that runs the inference on an entire video's sequence file\n",
    "\"\"\"\n",
    "\n",
    "GAUSSIAN_SIGMA = 2.0  # for smoothing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dc1d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model_from_checkpoint(\n",
    "    checkpoint_path: str,\n",
    "    input_size: int = 360,\n",
    "    hidden_size: int = 128,\n",
    "    num_layers: int = 2,\n",
    "    bidirectional: bool = True,\n",
    "    return_logits: bool = False,\n",
    "):\n",
    "    \"\"\"Load model weights from checkpoint, adapting architecture if needed.\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    # Extract model state dict\n",
    "    if isinstance(ckpt, dict) and 'model_state_dict' in ckpt:\n",
    "        state_dict = ckpt['model_state_dict']\n",
    "    elif isinstance(ckpt, dict) and any(k.startswith('lstm.') or k.startswith('fc.') for k in ckpt.keys()):\n",
    "        state_dict = ckpt\n",
    "    else:\n",
    "        # Fallback: attempt to use as state_dict\n",
    "        state_dict = ckpt\n",
    "\n",
    "    # Infer architecture from weights if possible\n",
    "    inferred_input_size = input_size\n",
    "    inferred_hidden_size = hidden_size\n",
    "    inferred_num_layers = num_layers\n",
    "    inferred_bidirectional = bidirectional\n",
    "\n",
    "    try:\n",
    "        # weight_ih_l0 shape: (4*hidden_size, input_size)\n",
    "        w_ih_l0 = state_dict.get('lstm.weight_ih_l0', None)\n",
    "        if w_ih_l0 is not None:\n",
    "            inferred_hidden_size = w_ih_l0.shape[0] // 4\n",
    "            inferred_input_size = w_ih_l0.shape[1]\n",
    "\n",
    "        # Determine num_layers by counting layers\n",
    "        layer_indices = set()\n",
    "        for k in state_dict.keys():\n",
    "            if k.startswith('lstm.weight_ih_l'):\n",
    "                try:\n",
    "                    idx_str = k.split('lstm.weight_ih_l')[1]\n",
    "                    idx = int(idx_str.split('_')[0]) if '_' in idx_str else int(idx_str)\n",
    "                    layer_indices.add(idx)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        if layer_indices:\n",
    "            inferred_num_layers = max(layer_indices) + 1\n",
    "\n",
    "        # Bidirectionality: presence of any reverse weights\n",
    "        inferred_bidirectional = any('_reverse' in k for k in state_dict.keys())\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Build model with inferred architecture\n",
    "    model = TennisPointLSTM(\n",
    "        input_size=inferred_input_size,\n",
    "        hidden_size=inferred_hidden_size,\n",
    "        num_layers=inferred_num_layers,\n",
    "        dropout=0.2,\n",
    "        bidirectional=inferred_bidirectional,\n",
    "        return_logits=return_logits,\n",
    "    )\n",
    "\n",
    "    # Load strictly now that shapes should match\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(\n",
    "        f\"Loaded checkpoint: {checkpoint_path} \"\n",
    "        f\"(input_size={inferred_input_size}, hidden_size={inferred_hidden_size}, \"\n",
    "        f\"num_layers={inferred_num_layers}, bidirectional={inferred_bidirectional})\"\n",
    "    )\n",
    "    return model, device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18cbb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'checkpoints/seq_len300/best_model.pth'\n",
    "model, device = load_model_from_checkpoint(model_path, bidirectional=True, return_logits=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb3e4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_feature_path = 'pose_data/features/yolos_0.25conf_15fps_0s_to_99999s/Brady Knackstedt (Blue Shirt⧸Black Shorts)(4.0 UTR) Unedited Match Play vs. opponent (5.54 UTR)_features.npz'\n",
    "video_feature_path = 'pose_data/features/yolos_0.25conf_15fps_0s_to_99999s/Unedited Matchplay vs UTR 9.8_features.npz'\n",
    "#video_feature_path = 'pose_data/features/yolos_0.25conf_15fps_0s_to_99999s/Holly Schlatter ｜ 2021 Australian Tennis Recruit  - Match Play Footage_features.npz'\n",
    "\n",
    "data = np.load(video_feature_path)\n",
    "targets = data['targets']\n",
    "# create our ordered list of sequences with 50% overlap: must carefully track frame numbers\n",
    "\n",
    "# Load fitted scaler and normalize features exactly like training\n",
    "scaler_path = 'data/seq_len_300/scaler.joblib'\n",
    "scaler = joblib.load(scaler_path)\n",
    "\n",
    "features = data['features']\n",
    "features_scaled = scaler.transform(features).astype(np.float32)\n",
    "\n",
    "num_frames = len(features_scaled)\n",
    "sequence_length = 300 \n",
    "overlap = 150\n",
    "if num_frames < sequence_length:\n",
    "    raise ValueError(\"input video too short\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8411ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idxs = []\n",
    "idx = 0\n",
    "while idx + sequence_length <= num_frames:\n",
    "    start_idxs.append(idx)\n",
    "    idx += overlap\n",
    "\n",
    "# If the last sequence doesn't reach the end, add one more sequence\n",
    "if start_idxs[-1] + sequence_length < num_frames:\n",
    "    start_idxs.append(num_frames - sequence_length)\n",
    "\n",
    "print(f\"Generated {len(start_idxs)} sequences for {num_frames} frames\")\n",
    "print(f\"Coverage: {start_idxs[0]} to {start_idxs[-1] + sequence_length}\")\n",
    "print(f\"Start indices: {start_idxs[:5]}...{start_idxs[-5:] if len(start_idxs) > 5 else start_idxs}\")\n",
    "\n",
    "# Check for gaps\n",
    "for i in range(len(start_idxs) - 1):\n",
    "    gap = start_idxs[i+1] - (start_idxs[i] + sequence_length)\n",
    "    if gap > 0:\n",
    "        print(f\"WARNING: Gap of {gap} frames between sequences {i} and {i+1}\")\n",
    "    elif gap < -overlap:\n",
    "        print(f\"WARNING: Excessive overlap of {-gap} frames between sequences {i} and {i+1}\")\n",
    "\n",
    "ordered_sequences = []\n",
    "output_arr = np.full((3, num_frames), np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1262cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# now we construct the feature lists, perform inference, and fill output array, tracking start indexes\n",
    "print(f\"Running inference on {len(start_idxs)} sequences...\")\n",
    "for seq_idx, i in enumerate(start_idxs):\n",
    "    # slice features and convert to tensor of shape (1, sequence_length, input_size)\n",
    "    seq_np = features_scaled[i:i+sequence_length, :]\n",
    "    seq_tensor = torch.from_numpy(seq_np).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output_tensor = model(seq_tensor)  # (1, seq_len, 1)\n",
    "    output_sequence = output_tensor.squeeze().detach().cpu().numpy()  # (seq_len,)\n",
    "    \n",
    "    # Find which row to place this sequence in\n",
    "    placed = False\n",
    "    for row in range(3):\n",
    "        if np.isnan(output_arr[row, i:i+sequence_length]).all():\n",
    "            output_arr[row, i:i+sequence_length] = output_sequence\n",
    "            if seq_idx < 5 or seq_idx >= len(start_idxs) - 5:  # Debug first/last few\n",
    "                print(f\"  Seq {seq_idx}: frames {i}-{i+sequence_length-1} -> row {row}\")\n",
    "            placed = True\n",
    "            break\n",
    "    \n",
    "    if not placed:\n",
    "        print(f\"ERROR: Could not place sequence {seq_idx} (frames {i}-{i+sequence_length-1})\")\n",
    "        raise ValueError('res arr filling logic messed up')\n",
    "\n",
    "# now we have filled res_arr. next, get 1, num_frames array by averaging over 0th axis, and apply gaussian smoothing\n",
    "print(\"Checking output_arr coverage...\")\n",
    "for row in range(3):\n",
    "    nan_count = np.isnan(output_arr[row, :]).sum()\n",
    "    print(f\"  Row {row}: {nan_count}/{num_frames} NaNs ({100*nan_count/num_frames:.1f}%)\")\n",
    "\n",
    "avg_probs = np.nanmean(output_arr, axis=0)\n",
    "nan_count_avg = np.isnan(avg_probs).sum()\n",
    "print(f\"avg_probs: {nan_count_avg}/{num_frames} NaNs ({100*nan_count_avg/num_frames:.1f}%)\")\n",
    "\n",
    "if nan_count_avg > 0:\n",
    "    # Find NaN ranges\n",
    "    nan_mask = np.isnan(avg_probs)\n",
    "    nan_starts = np.where(np.diff(np.concatenate(([False], nan_mask))))[0]\n",
    "    nan_ends = np.where(np.diff(np.concatenate((nan_mask, [False]))))[0]\n",
    "    print(\"NaN ranges:\")\n",
    "    for start, end in zip(nan_starts, nan_ends):\n",
    "        print(f\"  frames {start}-{end-1} ({end-start} frames)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d4d679",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_probs = scipy.ndimage.gaussian_filter1d(avg_probs.astype(np.float32), sigma=GAUSSIAN_SIGMA)\n",
    "_ = smoothed_probs  # silence variable display in notebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b43396",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"avg_probs head:\", avg_probs[:30])\n",
    "print(\"smoothed_probs head:\", smoothed_probs[:30])\n",
    "print(\n",
    "    \"smoothed stats:\",\n",
    "    \"min=\", float(np.nanmin(smoothed_probs)),\n",
    "    \"max=\", float(np.nanmax(smoothed_probs)),\n",
    "    \"nans=\", int(np.isnan(smoothed_probs).sum()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b6e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hysteresis_threshold(\n",
    "    values: np.ndarray,\n",
    "    low: float = 0.3,\n",
    "    high: float = 0.7,\n",
    "    min_duration: int = 0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Apply 1D hysteresis thresholding to a probability-like signal.\n",
    "\n",
    "    - Enter active state when values >= high\n",
    "    - Exit active state when values < low\n",
    "    - Optional min_duration suppresses short active segments\n",
    "    Returns a 0/1 array of the same length.\n",
    "    \"\"\"\n",
    "    assert 0.0 <= low < high <= 1.0, \"Require 0 <= low < high <= 1\"\n",
    "    n = len(values)\n",
    "    pred = np.zeros(n, dtype=np.int8)\n",
    "    active = False\n",
    "    start_idx: Optional[int] = None\n",
    "\n",
    "    for i in range(n):\n",
    "        v = values[i]\n",
    "        if not active:\n",
    "            if v >= high:\n",
    "                active = True\n",
    "                start_idx = i\n",
    "        else:\n",
    "            if v < low:\n",
    "                end_idx = i\n",
    "                if start_idx is not None and (end_idx - start_idx) >= max(0, min_duration):\n",
    "                    pred[start_idx:end_idx] = 1\n",
    "                active = False\n",
    "                start_idx = None\n",
    "\n",
    "    # Handle active segment reaching the end\n",
    "    if active and start_idx is not None:\n",
    "        end_idx = n\n",
    "        if (end_idx - start_idx) >= max(0, min_duration):\n",
    "            pred[start_idx:end_idx] = 1\n",
    "\n",
    "    return pred.astype(np.int32)\n",
    "\n",
    "# %%\n",
    "for high_thresh in range(50, 90, 5):\n",
    "    for low_thresh in range(10, 50, 5):\n",
    "        # now we compare output!\n",
    "        HIGH_THRESHOLD = high_thresh/100  # for starting a point\n",
    "        LOW_THRESHOLD =   low_thresh / 100 # for ending a point\n",
    "\n",
    "\n",
    "        filtered_sequence = hysteresis_threshold(smoothed_probs, LOW_THRESHOLD, HIGH_THRESHOLD, min_duration=6)\n",
    "        accuracy = np.sum(filtered_sequence == targets) / num_frames\n",
    "        print(f\"Accuracy: {accuracy:.3f}, High: {HIGH_THRESHOLD}, Low: {LOW_THRESHOLD} \")\n",
    "\n",
    "\n",
    "# %%\n",
    "np.unique(filtered_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c5c90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mask = (filtered_sequence > 0.5).astype(np.int32)\n",
    "accuracy = np.sum(acc_mask == targets) / num_frames\n",
    "accuracy\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# %%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6982002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tennis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
